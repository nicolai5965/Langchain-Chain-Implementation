{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNO/a5K5Mv7pL2ibCCAP0Pp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolai5965/Langchain-Chain-Implementation/blob/main/Langchain_Chain_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing packages"
      ],
      "metadata": {
        "id": "_s0uipOxkWuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ib3d1dpk6cKH"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import basic packages"
      ],
      "metadata": {
        "id": "dEt8Rv99keHx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pb_41jvv82Y0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import saved OpenAI api key"
      ],
      "metadata": {
        "id": "cemy7DNYkszd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hzNIH6gJEmXA"
      },
      "outputs": [],
      "source": [
        "def get_credentials_from_sheet(org_list):\n",
        "    # 1. Authorizing google colab\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    # 2. credentials for google sheets\n",
        "    import gspread\n",
        "    from google.auth import default\n",
        "    creds, _ = default()\n",
        "\n",
        "    # 3. authorizing the connection\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    # 4. Connecting\n",
        "    worksheet = gc.open('Credintials').sheet1\n",
        "\n",
        "    # 5. Exporting data to get_all_values gives a list of rows.\n",
        "    rows = worksheet.get_all_values()\n",
        "\n",
        "    # 6. Using pandas to convert to a DataFrame and render.\n",
        "    credentials = pd.DataFrame.from_records(rows)\n",
        "    # creating columns name\n",
        "    credentials.columns = credentials.iloc[0]\n",
        "    credentials = credentials.iloc[1:]\n",
        "\n",
        "    # Filter rows based on org_list\n",
        "    filtered_credentials = credentials[credentials['Organization'].isin(org_list)]\n",
        "\n",
        "    return filtered_credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMY7okJ7HFWb",
        "outputId": "fa1698b2-512c-41fa-8252-09bff6ab4494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 338 ms, sys: 24.3 ms, total: 362 ms\n",
            "Wall time: 6.07 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "orgs = ['OpenAI']\n",
        "credentials = get_credentials_from_sheet(orgs)\n",
        "\n",
        "def get_api_key(org_name, df=credentials):\n",
        "    return df.loc[df['Organization'] == org_name, 'API Key'].iloc[0]\n",
        "\n",
        "# Usage:\n",
        "openai_key = get_api_key('OpenAI')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Interpreter with Single Chain Processing"
      ],
      "metadata": {
        "id": "TUhfVtTFLBC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides a TextInterpreter_SingleChain class that processes a given review text through a single stage using the LangChain library."
      ],
      "metadata": {
        "id": "TtXaZskyLBGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "class TextInterpreter_SingleChain:\n",
        "    def __init__(self, openai_key):\n",
        "        # Define the ResponseSchema for each expected output\n",
        "        self.response_schemas = [\n",
        "            ResponseSchema(name=\"sentiment\", description=\"Is the text positive, neutral or negative? Only provide these words\"),\n",
        "            ResponseSchema(name=\"subject\", description=\"What subject is the text about? Use exactly one word.\"),\n",
        "            ResponseSchema(name=\"price\", description=\"How expensive was the product? Use None if no price was provided in the text\")\n",
        "        ]\n",
        "\n",
        "        # Create a StructuredOutputParser using the defined schemas\n",
        "        self.parser = StructuredOutputParser.from_response_schemas(self.response_schemas)\n",
        "\n",
        "        # Retrieve the format instructions from the parser\n",
        "        format_instructions = self.parser.get_format_instructions()\n",
        "\n",
        "        # Define the template using the format instructions\n",
        "        self.template = f\"\"\"\n",
        "\n",
        "        Just return the JSON, do not add ANYTHING, NO INTERPRETATION!\n",
        "\n",
        "        text: {{input}}\n",
        "\n",
        "        {{format_instructions}}\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize LangChain's OpenAI model with the provided API key\n",
        "        self.chat = ChatOpenAI(model='gpt-3.5-turbo-0613', openai_api_key=openai_key, temperature=0)\n",
        "\n",
        "    def interpret(self, text):\n",
        "        # Create a ChatPromptTemplate using the template\n",
        "        prompt = ChatPromptTemplate.from_template(template=self.template)\n",
        "\n",
        "        # Format the messages using the format_messages() method\n",
        "        messages = prompt.format_messages(input=text, format_instructions=self.parser.get_format_instructions())\n",
        "\n",
        "        # Get the response using the ChatOpenAI model\n",
        "        response = self.chat(messages)\n",
        "\n",
        "        # Parse the response content to get the structured output\n",
        "        output_dict = self.parser.parse(response.content)\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "    def print_settings(self):\n",
        "        # Print the default settings (attributes) of the ChatOpenAI instance\n",
        "        for attribute, value in self.chat.__dict__.items():\n",
        "            print(f\"{attribute}: {value}\")\n",
        "\n",
        "# Usage\n",
        "interpreter_SingleChain = TextInterpreter_SingleChain(openai_key)\n",
        "\n",
        "interpreter_SingleChain.print_settings()  # Call this to see the settings\n"
      ],
      "metadata": {
        "id": "HwAIZ42ANAUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a681ba-b108-403b-946b-8df01c41974a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache: None\n",
            "verbose: False\n",
            "callbacks: None\n",
            "callback_manager: None\n",
            "tags: None\n",
            "metadata: None\n",
            "client: <class 'openai.api_resources.chat_completion.ChatCompletion'>\n",
            "model_name: gpt-3.5-turbo-0613\n",
            "temperature: 0.0\n",
            "model_kwargs: {}\n",
            "openai_api_key: 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\n",
            "openai_api_base: \n",
            "openai_organization: \n",
            "openai_proxy: \n",
            "request_timeout: None\n",
            "max_retries: 6\n",
            "streaming: False\n",
            "n: 1\n",
            "max_tokens: None\n",
            "tiktoken_model_name: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of test prompts\n",
        "test_prompts = [\n",
        "    \"The new iPhone 13 costs $999 and it's absolutely amazing with its camera features!\",\n",
        "    #\"I went to the local bakery and bought a chocolate cake for $15. It tasted awful.\",\n",
        "    #\"Attending the concert last night was a blast, even though the tickets were a pricey $150 each.\",\n",
        "    #\"I recently read a book about space exploration. It was truly enlightening.\",\n",
        "    #\"The service at the downtown restaurant was terrible. I won't be going back there.\",\n",
        "    #\"Bought a pair of sneakers online for $65. They're comfortable and look stylish.\"\n",
        "]\n",
        "\n",
        "# Iterate over each prompt and get the model's interpretation\n",
        "for prompt in test_prompts:\n",
        "    result = interpreter_SingleChain.interpret(prompt)\n",
        "    print(f\"Input: {prompt}\")\n",
        "    print(f\"Output: {result}\\n\")\n"
      ],
      "metadata": {
        "id": "hP9jv4csNOCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cef676-11f5-4a3b-8f6d-e2d6115b8caa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The new iPhone 13 costs $999 and it's absolutely amazing with its camera features!\n",
            "Output: {'sentiment': 'positive', 'subject': 'iPhone', 'price': '$999'}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Interpreter with Multi-Chain Processing\n"
      ],
      "metadata": {
        "id": "Fl82kln0KSCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides a TextInterpreter_MultiChain class that processes a given review text through multiple stages using the LangChain library."
      ],
      "metadata": {
        "id": "RefaWulHKTK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain, SequentialChain\n",
        "\n",
        "\n",
        "class TextInterpreter_MultiChain:\n",
        "    def __init__(self, openai_key, word_limit=50):\n",
        "        self.word_limit = word_limit\n",
        "\n",
        "        # Initialize LangChain's OpenAI model with the provided API key\n",
        "        self.chat = ChatOpenAI(model='gpt-3.5-turbo-0613', openai_api_key=openai_key, temperature=0)\n",
        "\n",
        "        # Initialize chains\n",
        "        self._initialize_chains()\n",
        "\n",
        "    def _get_concise_instruction(self):\n",
        "        return f\"Provide a concise answer, ideally within {self.word_limit} words, ensuring it's complete and makes sense.\"\n",
        "\n",
        "    def _initialize_chains(self):\n",
        "        # Chain to determine the sentiment of the review\n",
        "        self.chain_sentiment = LLMChain(\n",
        "            llm=self.chat,\n",
        "            prompt=PromptTemplate.from_template(\"Determine the sentiment of the review: {review_text}. (positive, negative, neutral)\"),\n",
        "            output_key=\"sentiment\"\n",
        "        )\n",
        "\n",
        "        # Chain to determine the subject of the review\n",
        "        self.chain_subject = LLMChain(\n",
        "            llm=self.chat,\n",
        "            prompt=PromptTemplate.from_template(\"Identify the main subject of the review: {review_text}. (one word)\"),\n",
        "            output_key=\"subject\"\n",
        "        )\n",
        "\n",
        "        # Chain to extract the price mentioned in the review, if any\n",
        "        self.chain_price = LLMChain(\n",
        "            llm=self.chat,\n",
        "            prompt=PromptTemplate.from_template(\"Extract the price mentioned in the review: {review_text}. (None if no price mentioned)\"),\n",
        "            output_key=\"price\"\n",
        "        )\n",
        "\n",
        "        # Other chains as previously defined\n",
        "        concise_instruction = self._get_concise_instruction()\n",
        "        self.chain_review = LLMChain(\n",
        "            llm=self.chat,\n",
        "            prompt=PromptTemplate.from_template(f\"Analyze the sentiment of the following review: {{review_text}}. {concise_instruction}\"),\n",
        "            output_key=\"detailed_sentiment\"\n",
        "        )\n",
        "\n",
        "        self.chain_comment = LLMChain(\n",
        "            llm=self.chat,\n",
        "            prompt=PromptTemplate.from_template(f\"Given the sentiment of the review: {{detailed_sentiment}}. {concise_instruction}\"),\n",
        "            output_key=\"comment\"\n",
        "        )\n",
        "\n",
        "        self.chain_follow_up = LLMChain(\n",
        "            llm=self.chat,\n",
        "            prompt=PromptTemplate.from_template(f\"Write a follow-up comment on the {{review_text}}. {concise_instruction}\"),\n",
        "            output_key=\"follow-up\"\n",
        "        )\n",
        "\n",
        "        self.chain_summary = LLMChain(\n",
        "            llm=self.chat,\n",
        "            prompt=PromptTemplate.from_template(f\"Summarise the {{review_text}} and our {{comment}} in one concise sentence. {concise_instruction}\"),\n",
        "            output_key=\"summary\"\n",
        "        )\n",
        "\n",
        "        self.chain_improvements = LLMChain(\n",
        "            llm=self.chat,\n",
        "            prompt=PromptTemplate.from_template(f\"From the review, suggest main improvements in one concise sentence. {concise_instruction}\"),\n",
        "            output_key=\"improvements\"\n",
        "        )\n",
        "\n",
        "        self.overall_chain = SequentialChain(\n",
        "            chains=[self.chain_sentiment, self.chain_subject, self.chain_price, self.chain_review, self.chain_comment, self.chain_summary, self.chain_improvements, self.chain_follow_up],\n",
        "            input_variables=[\"review_text\"],\n",
        "            output_variables=[\"sentiment\", \"subject\", \"price\", \"detailed_sentiment\", \"comment\", \"summary\", \"improvements\", \"follow-up\"]\n",
        "        )\n",
        "\n",
        "    def multi_chain_interpret(self, review_text):\n",
        "        return self.overall_chain({\"review_text\": review_text})\n",
        "\n",
        "    def print_settings(self):\n",
        "        # Print the default settings (attributes) of the ChatOpenAI instance\n",
        "        for attribute, value in self.chat.__dict__.items():\n",
        "            print(f\"{attribute}: {value}\")\n",
        "\n",
        "# Create an instance of the TextInterpreter_MultiChain class\n",
        "interpreter_MultiChain = TextInterpreter_MultiChain(openai_key, word_limit=50)\n",
        "\n",
        "# Multi-chain usage\n",
        "multi_chain_result = interpreter_MultiChain.multi_chain_interpret(\"A bit disappointed with the recent purchase. Noticed some crack in the wood of the table. They refused to exchange it.\")\n",
        "display(multi_chain_result)\n"
      ],
      "metadata": {
        "id": "cQ9MrFt5NDyA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a3dd4272-a03d-4923-83e3-00aa9db23e1c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'review_text': 'A bit disappointed with the recent purchase. Noticed some crack in the wood of the table. They refused to exchange it.',\n",
              " 'sentiment': 'negative',\n",
              " 'subject': 'table',\n",
              " 'price': 'None',\n",
              " 'detailed_sentiment': \"The sentiment of the review is negative. The customer expresses disappointment with their recent purchase, specifically mentioning cracks in the table's wood and the refusal of the store to exchange it.\",\n",
              " 'comment': \"The customer expresses disappointment with their recent purchase, mentioning cracks in the table's wood and the store's refusal to exchange it.\",\n",
              " 'summary': \"The customer is disappointed with their recent purchase due to cracks in the table's wood and the store's refusal to exchange it.\",\n",
              " 'improvements': 'Improve the customer service by addressing and resolving issues promptly and efficiently.',\n",
              " 'follow-up': \"I'm sorry to hear about your disappointment with the purchase. It's unfortunate that they refused to exchange the table despite the cracks in the wood. Have you considered reaching out to their customer service or exploring any warranty options?\"}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment-Based Text Interpretation using Multi-Chains"
      ],
      "metadata": {
        "id": "u6HhcW1FKoZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code introduces the SentimentBasedTextInterpreter class, which interprets text based on its sentiment (positive, neutral, or negative) using multiple processing chains from the LangChain library.\n",
        "\n"
      ],
      "metadata": {
        "id": "K_VbuYLRKq18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "\n",
        "\n",
        "class SentimentBasedTextInterpreter:\n",
        "    \"\"\"Class to interpret text using chains chosen based on sentiment (positive, neutral, negative).\"\"\"\n",
        "    def __init__(self, model_name, api_key, temperature=0, word_limit=50):\n",
        "        \"\"\"Initialize the interpreter with the given settings.\"\"\"\n",
        "        self.llm = ChatOpenAI(model=model_name, openai_api_key=api_key, temperature=temperature)\n",
        "        self.word_limit = word_limit\n",
        "        self._initialize_chains()\n",
        "\n",
        "    def _initialize_chains(self):\n",
        "        \"\"\"Set up the chains for text interpretation based on sentiment.\"\"\"\n",
        "        self._initialize_destination_chains()\n",
        "        self._initialize_router_chain()\n",
        "        self._initialize_multi_prompt_chain()\n",
        "\n",
        "    def _initialize_destination_chains(self):\n",
        "        \"\"\"Initialize the destination chains for positive, neutral, and negative sentiments.\"\"\"\n",
        "        self.destination_chains = {}\n",
        "        for p_info in self._get_prompt_infos():\n",
        "            name = p_info[\"name\"]\n",
        "            prompt_template = p_info[\"prompt_template\"]\n",
        "            prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
        "            chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "            self.destination_chains[name] = chain\n",
        "\n",
        "    def _initialize_router_chain(self):\n",
        "        \"\"\"Initialize the router chain that decides which destination chain to use.\"\"\"\n",
        "        destinations = [f\"{p['name']}: {p['description']}\" for p in self._get_prompt_infos()]\n",
        "        destinations_str = \"\\n\".join(destinations)\n",
        "        router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "        router_prompt = PromptTemplate(\n",
        "            template=router_template,\n",
        "            input_variables=[\"input\"],\n",
        "            output_parser=RouterOutputParser(),\n",
        "        )\n",
        "        self.router_chain = LLMRouterChain.from_llm(self.llm, router_prompt)\n",
        "\n",
        "    def _initialize_multi_prompt_chain(self):\n",
        "        \"\"\"Initialize the multi-prompt chain that uses the router and destination chains.\"\"\"\n",
        "        self.chain = MultiPromptChain(\n",
        "            router_chain=self.router_chain,\n",
        "            destination_chains=self.destination_chains,\n",
        "            default_chain=self.destination_chains[\"neutral\"],\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "\n",
        "    def _get_prompt_infos(self):\n",
        "        \"\"\"Return the prompt information for different sentiments.\"\"\"\n",
        "\n",
        "        positive_template = \"\"\"You are an AI that focuses on the positive side of things. \\\n",
        "                              Whenever you analyze a text, you look for the positive aspects and highlight them. \\\n",
        "                              Here is the text:\n",
        "                              {input}\"\"\"\n",
        "        neutral_template = \"\"\"You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, \\\n",
        "                              not favoring any positive or negative aspects. Here is the text:\n",
        "                              {input}\"\"\"\n",
        "        negative_template = \"\"\"You are an AI that is designed to find the negative aspects in a text. \\\n",
        "                              You analyze a text and show the potential downsides. Here is the text:\n",
        "                              {input}\"\"\"\n",
        "        return [\n",
        "            {\n",
        "                \"name\": \"positive\",\n",
        "                \"description\": \"Good for analyzing positive sentiments\",\n",
        "                \"prompt_template\": positive_template,\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"neutral\",\n",
        "                \"description\": \"Good for analyzing neutral sentiments\",\n",
        "                \"prompt_template\": neutral_template,\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"negative\",\n",
        "                \"description\": \"Good for analyzing negative sentiments\",\n",
        "                \"prompt_template\": negative_template,\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    def multi_chain_interpret(self, text):\n",
        "        \"\"\"Interpret the given text using the appropriate chain based on sentiment and return a response.\"\"\"\n",
        "\n",
        "        output = self.chain.run(text)\n",
        "        simple_prompt = PromptTemplate(template=\"{input}\", input_variables=[\"input\"])\n",
        "        simple_chain = LLMChain(llm=self.llm, prompt=simple_prompt)\n",
        "        response_prompt = f\"Based on the following points: {output}, make a respond to the customer. The respond should only be {self.word_limit} words long.\"\n",
        "        return simple_chain.run(response_prompt)\n",
        "\n",
        "    def print_settings(self):\n",
        "        \"\"\"Print the settings of the ChatOpenAI instance.\"\"\"\n",
        "        for attribute, value in self.llm.__dict__.items():\n",
        "            print(f\"{attribute}: {value}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interpreter = SentimentBasedTextInterpreter(model_name='gpt-3.5-turbo-0613', api_key=openai_key, word_limit=50)\n",
        "    interpreter.print_settings()\n",
        "    text = (\"I ordered Pizza Salami for 9.99$ and it was a bit bland, \"\n",
        "            \"there where a lot of salami on it, which where god, but there where too little sauce and almost no cheese\")\n",
        "    response = interpreter.multi_chain_interpret(text)\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh8MkTuZGErl",
        "outputId": "96f7dbe8-32cd-4610-c6c3-c41c5428532f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache: None\n",
            "verbose: False\n",
            "callbacks: None\n",
            "callback_manager: None\n",
            "tags: None\n",
            "metadata: None\n",
            "client: <class 'openai.api_resources.chat_completion.ChatCompletion'>\n",
            "model_name: gpt-3.5-turbo-0613\n",
            "temperature: 0.0\n",
            "model_kwargs: {}\n",
            "openai_api_key: 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\n",
            "openai_api_base: \n",
            "openai_organization: \n",
            "openai_proxy: \n",
            "request_timeout: None\n",
            "max_retries: 6\n",
            "streaming: False\n",
            "n: 1\n",
            "max_tokens: None\n",
            "tiktoken_model_name: None\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative: {'input': 'I ordered Pizza Salami for 9.99$ and it was a bit bland, there where a lot of salami on it, which where god, but there where too little sauce and almost no cheese'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "We apologize for the blandness of the pizza and the lack of sauce and cheese. We appreciate your feedback and will work on improving these aspects. Thank you for bringing this to our attention.\n"
          ]
        }
      ]
    }
  ]
}